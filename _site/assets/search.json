

[
  
  
    
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Reading",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Stay Tuned, Coming Soon\n\nWe are in the midst of uploading all of our files onto Amazon Web Services. We will post an announcement once\nit is up and ready to go!\n",
    "url": "/downloads/"
  },
  
  {
    "title": "Description",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "SRI International and Lab41, In-Q-Tel, are proud to release the Voices Obscured in Complex Environmental Settings (VOICES) corpus, a collaborative effort that brings speech data in acoustically challenging reverberant environments to the researcher. Clean speech was recorded in rooms of different sizes, each having distinct room acoustic profiles, with background noise played concurrently. These recordings provides audio data that better represent real-use scenarios. The intended purpose of this corpus is to promote acoustic research including, but not limited to:\n\n\n  speaker Identification, speech recognition, speaker detection\n  event and background classification, speech/non-speech\n  source separation and localization, noise reduction, general enhancement, acoustic quality metrics\n\n\nThe corpus contains the source audio, the retransmitted audio, orthographic transcriptions, and speaker labels. The ultimate goal of this corpus is to advance acoustic research by providing access to complex acoustic data. The corpus will be released as open source, Creative Commons BY 4.0, free for commercial, academic, and government use.\n\n\n\nDataset Details\n\nThis is one of the largest corpora to date that has transcriptions and simulatenously recorded real-world noise. The details:\n\n\n  Source Material: a total of 15 hours (3,903 audio files)\n  Language audio contains English read speech with male and females\n  Simulated Head Movement the loudspeaker playing the foreground speech was on a motorized rotating platform\n  Distractor Noise a large collection containing television, music, babble noise, and HVAC at various SNR\n  Multiple Rooms large, medium, and small, with various reverberation\n\n\nMore specific details can be seen at in our readme and paper in the reading section\n\n",
    "url": "/"
  },
  
  {
    "title": "Description",
    "excerpt": "Links and Contacts\n",
    "content": "Competitions\n\n  Coming Soon!\n\n\nSponsoring Organizations\n\n  \n \n  \n\n\nParticipating Organizations\n\n  MIT Lincoln Laboratory\n  Amazon Web Services\n\n\n",
    "url": "/links/"
  },
  
  {
    "title": "Organizers",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Zeb Armstrong\n\n\nZeb Armstrong is a Computer Scientist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. His main interest is preparing and transitioning speech technologies to fulfill\nthe needs of government and commercial users through software testing and user interface\ndesign. He uses his experience with operational signals collection and audio processing to\nanticipate clients’ needs and inform design decision.  Armstrong studied software engineering at \nClarkson University before joining the US Army in 2013. While in the Army he worked as an\nArabic linguist and signals intelligence analyst, serving in both tactical and strategic capacities.\n\nMaria Alejandra Barrios\n\n\nDr. Maria Alejandra Barrios is a data scientist at Lab41, In-Q-Tel. Her research\ninterests include deep learning implementations for audio signal processing,\ntime-series analysis, image processing, and computer vision. Dr. Barrios received\nher PhD. in plasma physics from University of Rochester in 2010. Prior to joining\nLab41 Dr. Barrios was a staff scientist at Lawrence Livermore National Laboratory, working\non laser-driven fusion experiments at the NIF laser facility.\n\nHoracio Franco\n\n\nHoracio Franco is Chief Scientist in SRI International’s Speech Technology and Research (STAR)\nLaboratory. Since joining SRI in 1990, he has contributed in the areas of acoustic modeling,\nhybrid Hidden Markov Model (HMM)-neural net speech recognition approaches, speech\nrecognizer architectures, speech technology for language learning, noise-robust speech\nrecognition, and speech-to- speech translation systems. He has co-authored more than 70\npapers, as well as several communications and book chapters. He holds several U.S. patents. He\nhas also been active in leading efforts to develop and deploy SRI's speech technology in\ndifferent areas of government and commercial use. His Ph.D. in engineering is from the\nUniversity of Buenos Aires, Argentina.\n\nPaul Gamble\n\n\nDr. Paul Gamble studied Biophysics at the University of Pennsylvania and received an MD from Washington University in St. Louis where he studied nerve-computer interfaces. He joined Lab 41 as a Data Scientist in September 2017 and is helping to develop a pipeline for classifying synthetic DNA sequences.e\n\nMartin Graciarena\n\n\nMartin Graciarena is a Technical Manager in SRI International’s Speech Technology and\nResearch (STAR) Laboratory. His research interests include noise robust features; voice activity\ndetection; speaker, language, and speech recognition; non-audible microphone signal\nprocessing; and bird song processing. He has more than 30 publications in peer-reviewed\nconferences and holds two patents. His Ph.D. in noise robust speech processing is from the\nUniversity of Buenos Aires, Argentina.\n\nJeff Hetherly\n\n\nDr. Jeff Hetherly Dr. Jeff Hetherly is a data scientist at In-Q-Tel’s Lab41. His background is in scientific statistical analysis, methods for dimensionality reduction, and machine learning. Dr. Hetherly received his Ph.D. in experimental particle physics from Southern Methodist University where he spent two years located at CERN analyzing data for the ATLAS experiment.\n\nAaron Lawson\n\n\nAaron Lawson is Assistant Director of SRI International’s Speech Technology and Research\n(STAR) Laboratory. His research interests include voice forensics and biometrics, languageand speaker identification from speech, social media information extraction, noise robustness,and fielding systems. He has published more than 30 papers covering speech, natural language\nand linguistics. His Ph.D. in applied linguistics is from Cornell University.\n\nKarl Ni\n\n\nDr. Karl Ni is the Technical Director of Lab41, In-Q-Tel’s AI and machine learning analytics laboratory. His interests lie in computer vision, audio signal processing, speech recognition, and natural language processing. Prior to In-Q-Tel, Dr. Ni served as principle investigator and program manager on laboratory directed projects at federally funded research and development centers, both MIT Lincoln Laboratory and Lawrence Livermore National Laboratory. Projects started and led at these organizations resided under the President’s BRAIN initiative, the Departments of Defense and Homeland Security, and internally funded research. Dr. Ni received his doctoral degree from the University of California, San Diego in Pattern Recognition Techniques for Image Processing, and he received his bachelors degree from the University of California at Berkeley.\n\nColleen Richey\n\n\nColleen Richey is an Advanced Linguist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. Her research interests include automatic recognition of conversational\nspeech, machine translation, language education, and speech and language technology for low-\nresource languages. She has published more than 20 papers in speech technology and machine\ntranslation. Her M.A. in linguistics is from Stanford University.\n\nAllen Stauffer\n\n\nAllen Stauffer is a Computer Scientist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. His main focus is preparing speech processing technologies for commercial\nand government transition, including software and hardware testing and validation, system\nevaluation and preparation of relevant data to meet client needs. A second facet of his work is\nin developing speech processing system requirements based on his understanding of client\nneeds and interactions with the operational community. His M.A. in computer science is from\nthe University of Texas at Dallas.\n\nCory Stephenson\n\n\nDr. Cory Stephenson is a data scientist at Lab41, In-Q-Tel.  His background is in complex systems and non-equilibrium thermodynamics.  His most recent work is in the application of machine learning and deep learning methods to audio signal processing. Dr. Stephenson received his Ph.D. from the University of Illinois at Urbana-Champaign.\n\n",
    "url": "/organizers/"
  },
  
  {
    "title": "Reading",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Dataset Description\n\nThe Voices Obscured in Complex Environmental settings (VOiCES) corpus presents audio \nrecorded in acoustically challenging conditions. Recordings took place in real rooms of \nvarious sizes, capturing different background and reverberation profiles for each \nroom. Various types of distractor noise (TV, music, or babble) were simultaneously \nplayed with clean speech. Audio was recorded at a distance using twelve microphones \nstrategically placed throughout the room. To imitate human behavior during conversation, \nthe foreground speaker used a motorized platform to rotated over a range of angles during recordings.\n\nData format\n\nAudio files will be available in WAV format with 16 kHz sample rate with 16-bit precision. All files begin with the corpus name Lab41-SRI-VOiCES. Source files specify speaker, chapter, and chapter segment identification number. The file naming format sample is shown below:\n\n  Lab41-SRI-VOiCES-scr-sp&lt; speaker_ID &gt;-ch&lt; chapter_ID &gt;-sg&lt; segmetn_ID &gt;.wav\n\n\nNaming convention for audio at a distance include all the above information, with additional descriptors for room, distractor noise, microphone type, microphone location, and position of foreground speaker in degrees. The file naming format is shown below:\n\n  Lab41-SRI-VOiCES-&lt; room &gt;-&lt; distractor_noise &gt;-sp&lt; speaker_ID &gt;-ch&lt; chapter_ID &gt;-seg&lt; segment_ID &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;-dg&lt; degree &gt;.wav\n\n\n\n\nPossible descriptors for room, distractor noise, microphone type, and micrphone location, are show in the table.\n\n\n\n\n  \n    \n      File Code\n      Type\n      Definition\n    \n  \n  \n    \n      rm1\n      Room\n      Room-1: dimensions 146” x 107” (x 107” height)\n    \n    \n      rm2\n      Room\n      Room-2: dimensions 225” x 158” (x 109” height)\n    \n    \n      scr\n      Source audio\n      Source audio for foreground speaker\n    \n    \n      none\n      Distractor noise\n      No distractor noise played\n    \n    \n      musi\n      Distractor noise\n      Music distractor noise played\n    \n    \n      tele\n      Distractor noise\n      Television distractor noise played\n    \n    \n      babb\n      Distractor noise\n      Babble distractor noise played\n    \n    \n      stu\n      Mic type\n      Cardioid dynamic studio microphone\n    \n    \n      lav\n      Mic type\n      Omnidirectional condenser lavalier microphone\n    \n    \n      clo\n      Mic location\n      Closest to foreground speaker- on table\n    \n    \n      mid\n      Mic location\n      Mid-distance to foreground speaker- on table\n    \n    \n      far\n      Mic location\n      Farthest to foreground speaker- on stand\n    \n    \n      beh\n      Mic location\n      Behind foreground speaker- on stand\n    \n    \n      cec\n      Mic location\n      Overhead on ceiling, clear\n    \n    \n      ceo\n      Mic location\n      Overhead on ceiling, fully obstructed\n    \n    \n      tbo\n      Mic location\n      Partially obstructed - table\n    \n    \n      wlo\n      Mic location\n      Fully obstructed - wall\n    \n    \n      impulse\n      Signal\n      Two seconds with transient sound in middle, for room response\n    \n    \n      swoop\n      Signal\n      Rising tone for 20 seconds, for room response\n    \n    \n      tone\n      signal\n      Steady tone for 15 seconds, for room response\n    \n  \n\n\n\n\nMicrophone identification numbers are unique to a specific microphone location and type, defined below.\n\n\n\n\n  \n    \n      Mic_ID\n      Location\n      Type\n    \n  \n  \n    \n      01\n      clo\n      stu\n    \n    \n      02\n      clo\n      lav\n    \n    \n      03\n      mid\n      stu\n    \n    \n      04\n      mid\n      lav\n    \n    \n      05\n      far\n      stu\n    \n    \n      06\n      far\n      lav\n    \n    \n      07\n      beh\n      stu\n    \n    \n      08\n      beh\n      lav\n    \n    \n      09\n      tbo\n      lav\n    \n    \n      10\n      cec\n      lav\n    \n    \n      11\n      ceo\n      lav\n    \n    \n      12\n      wlo\n      lav\n    \n  \n\n\n\n\nAudio files to characterize the room response are also available:\n\n  Lab41-SRI-VOiCES-&lt; room &gt;-&lt; signal &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;.wav\n\n\nAs are recordings of distactor noise only or ambient room background only:\n\n  Lab41-SRI-VOiCES-&lt; distractor_noise &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;.wav\n\n\nBlog Posts\n\n\n  Introducing the Voices Obscured in Complex Environmental Settings (VOiCES) corpus\n\n\nPublications\n\n\n  Corpus Description and Collection Methodology\n  Abstract for ASA – Coming Soon\n  Poster for ASA – Coming Soon\n\n\n",
    "url": "/reading/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

