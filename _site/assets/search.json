

[
  
  
    
    
      {
        "title": "VOiCES Appearing At ICASSP 2018",
        "excerpt": "Lab41 and SRI International will be attending the International Conference of Acoustics Speech and Signal Processing at 2018. We will be handing out VOiCES flyers and talking with prospectives as well as promoting the corpus. Come visit us at the booths!\n",
        "content": "\nLab41 and SRI International will be attending the International Conference of Acoustics Speech and Signal Processing at 2018, April 15. We will be handing out VOiCES flyers and talking with prospectives as well as promoting the corpus. Come visit us at the booths!\n\nThe IEEE ICASSP 2018 conference will be held from April 15-20, 2018 in Calgary, Canada. This year, their theme is “Signal Processing and Artificial Intelligence: Changing the World”.\n\n",
        "url": "/general/2018/04/15/icassp-2018/"
      },
    
      {
        "title": "VOiCES Appearing At ASA 2018",
        "excerpt": "Lab41 and SRI International will be presenting at the Acoustical Society of America’s annual meeting in May 7-11, 2018. We will be available for questions at our poster session.\n",
        "content": "\nLab41 and SRI International will be attending the Acoustical Society of America at 2018, May 7-11. We will be presenting our work in presentation. Details about our corpus will be provided. We will also be handing out VOiCES flyers and talking with prospectives as well as promoting the corpus. Come visit our poster.\n\nAbstract: \nVoices Obscured in Complex Environmental Setting (VOiCES) corpus is a collaboration between Lab41 and SRI International, designed to be a freely available data set for speech and acoustics research in noisy room conditions.  The main focus of the corpus is on distant microphone collection in a series of four rooms of different sizes and configurations. There are both foreground speech and background adversarial sounds, played through high-quality speakers in each room to create multiple, realistic acoustic environments. The foreground speech is played from a randomly rotating speaker to emulate head motion.  Foreground speech consists of approximately 15 hours of audio from LibriVox audio collections per session per microphone and the background distractor sounds will consist of babble, music, HVAC, TV/radio, dogs, vehicles and weather sounds drawn from the MUSAN collection. Each room has multiple sessions to exhaustively cover the background foreground combinations, and the audio is collected with twelve different microphones (omnidirectional lavalier, studio cardioid and piezoelectric) placed strategically around the room. The resulting data set was designed to enable acoustic research on event detection, background detection, source separation, speech enhancement, source distance, sound localization, as well as speech research on speaker recognition, speech activity detection, speech recognition and language recognition.\n\n\n\nThe Acoustical Society of America Meeting was held from May 7-11, 2018 in Minneapolis, Minnesota.\n\n",
        "url": "/general/2018/05/07/asa-2018/"
      },
    
      {
        "title": "VOiCES Appearing At Interspeech 2018",
        "excerpt": "Lab41 and SRI International will be presenting at the Interspeech annual meeting in September 2-6, 2018. We will be available for questions at our poster session.\n",
        "content": "\nLab41 and SRI International will be attending Interspeech 2018, to be hosted in Hyderabad, India September 2-6. We will be presenting details of the corpus as well as outlining results from model baselines when using VOiCES. Come visit our poster.\nConference publication available here.\n\nAbstract:\nRecent advances in speech and signal processing research leverage deep learning frameworks that can model data complexity better than more traditional approaches. Publicly available audio corpora that are large enough for deep learning implementations are mostly composed of isolated speech at close range microphony. A typical approach to better represent realistic scenarios, is to convolve clean speech with noise and simulated room response for model training. Despite these efforts model performance significantly degrades when tested against un-curated data collected in the wild. This paper introduces the Voices Obscured In Complex Environmental Settings (VOICES) corpus, a freely available dataset under Creative Commons BY 4.0. This dataset will promote speech and signal processing research under noisy room conditions for speakers at a distance. Data was recorded in furnished rooms with distractor noise played in conjunction with isolated speech. Multiple sessions were recorded in each room to accommodate for all foreground speech-distractor noise combinations. Audio was recorded using twelve microphones placed throughout the room, resulting in 120 hours of recordings per microphone. This work is a multi-organizational effort led by SRI International and Lab41 with the intent to push forward state of the art distant microphone approaches in signal processing and speech recognition.\n\n\n\nInterspeech 2018 was held September 2-6, 2018 in Hyderabad, India.\n",
        "url": "/general/2018/09/01/interspeech-2018/"
      },
    
      {
        "title": "Interspeech 2019 Special Session",
        "excerpt": "Participate in the VOiCES from a Distance Challenge and present results at the Interspeech 2019\nspecial session.\n",
        "content": "\n\nSRI International and Lab41 will be hosting a special session at Interspeech 2019:\nThe VOiCES from a Distance Challenge.  This challenge will focus on benchmarking and further improving state-of-the-art technologies in the area of speaker recognition and automatic speech recognition (ASR) for far-field speech. VOiCES data from room-1 and room-2 will be available for participants to use as\ntraining or development data. The final challenge evaluation will be done with data from room-3 and\nroom-4. You can find more information on the challenge here.\n",
        "url": "/general/2019/01/10/Interspeech2019SpecialSession/"
      },
    
  
  
  
  {
    "title": "Description",
    "excerpt": "Interspeech 2019 Special Session\n",
    "content": "Please visit Challenge page\n",
    "url": "/Interspeech2019-Special-Session/"
  },
  
  {
    "title": "Description",
    "excerpt": "Interspeech 2019 Special Session\n",
    "content": "The Challenge\n\nThe VOiCES from a distance challenge 2019 will be focused on benchmarking and further improving state-of-the-art technologies\nin the area of speaker recognition and automatic speech recognition (ASR) for far-field speech. The challenge will use the VOiCES data for development and evaluation. The challenge will have two tracks for speaker recognition and ASR:\n\n(i) Fixed System - Training data is limited to a subset of VOiCES, a subset of LibriSpeech, SITW, and VoxCeleb\n\n(ii) Open System - Participants can use any external datasets they have access to (private or public)\n\nREGISTER HERE\n \n\n\n\n  Participants who complete the challenge (submit their system outputs and system description) will get early access to the VOiCES phase 2 data. \nThe phase 2 data is an extension of VOiCES, with over 310k audio files recorded in new reverberant environments.\n\n\nUpdates and News\nJan. 24, 2019\n\n  The official scorer for speaker recognition is now released\n  Updated ASR development set is released.\n  Updated evaluation plan is released.\n\n\nData and Evaluation\nData subsets for challenge tracks are available for download HERE. Make sure to check-out the README.VOICES_2019.txt for more details on what is included in each subset and tips on how to to unzip the data.\n\nThe evaluation plan outlines training conditions for fixed and open tracks, details on the development and evaluation data, model performance metrics, and how to submit scores, for both ASR and SID tasks. This document also provides information on how the training and evaluation datasets provided for download are organized, and evaluation rules for the challenge.\n\nTimeline\nJan. 15, 2019 : Release of the evaluation plan and development sets\nFeb. 25, 2019 : Evaluation data available\nMarch 4, 2019 :  System output submission deadline (11:59 PM PST)\nMarch 11, 2019 : Release of evaluation results\nMarch 15, 2019 : System description submission and release of VOiCES phase II key for the participating teams\nMarch 29, 2019 : Regular paper submission deadline for Interspeech 2019\n\nFAQ’s\n\nQ1. Am I allowed to use additional “non-speech” data for data augmentation by mixing them with clean training\ndata you provided for the fixed condition?\nAs specified in the evaluation plan, you are allowed to use any “non-speech” data for data augmentation\nin the fixed condition for ASR and speaker recognition.\n\nQ2. I cannot download the data, the website is blocked by my institution or country. How can I get the\ndata?\nPlease send an email to voices_poc@sri.com\nand we will create an ftp link for you to download the data.\n\nQ3. Is there a registration deadline?\nParticipants may register at any time before March 4th.\n\nQ4. Will the accepted paper submitted for the challenge be published in the conference proceedings?\nYes, the submitted paper will go through a peer review process and will be part of the official Interspeech\nconference proceedings.\n\nQ5. Will there be metadata available in the evaluation set (speaker, room, microphone, etc) for each utterance (as in the dev set)?\nWe only provide metadata in the dev set as a way for the participants to look at the performance of\ntheir system under different microphones, rooms or distractors. The metadata will not be provided for the evaluation set, in fact, we anticipate the file names will be fully anonymized.\n\nQ6. We are allowed to use VoxCeleb1, VoxCeleb2 and SITW data for training. Are we limited to using the official annotations only? For example, the VoxCeleb corpora contain segments from YouTube videos. Are we limited to using only the annotated segments or may we use other parts of the videos also, if we find it useful?\nThe audio data from the Voxceleb1 and Voxceleb2 is restricted to the official annotations for the fixed condition submissions. In this way, the fixed condition can serve its purpose of measuring the performance of different systems trained with the same data (or a subset thereof). However, you can certainly use all the audio from the Voxceleb-provided URLs for\nthe open condition system development.\n\nQ7. How are we supposed to submit our system description and score files?\nWe will provide you instructions along with the release of the evaluation data on how to upload the\nsystem description and score files.\n\nQ8. For the ASR task, may we use an external pronunciation lexicon (e.g., CMUdict) for deriving the pronunciation of words in the training data?\nYes, you may use any dictionary you have for the purpose of this evaluation, in either fixed or open\ncondition.\n\nQ9. How will the team submission be ranked?\nThe official score for a team will be selected as the best primary metric from systems submitted by\nthe team for that condition (up to 3 systems can be submitted per condition per task per team). These official scores will be used for ranking teams.\n\nAdditional questions about the challenge? contact us at voices_poc@sri.com\n",
    "url": "/Interspeech2019_SpecialSession/"
  },
  
  {
    "title": "About",
    "excerpt": "\n",
    "content": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com\n\nYou can find the source code for Minima at GitHub:\njekyll /\nminima\n\nYou can find the source code for Jekyll at GitHub:\njekyll /\njekyll\n\n",
    "url": "/about/"
  },
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Reading",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Dataset Download\n\nVOiCES is now publicly available on AWS. Check out the AWS Registry of Open Data for details. Data structure\nand microphone specifics are available in our README file.\n\nDependencies\n\nTo download VOiCES audio corpus, install the\nAWS Command Line Interface (CLI) and link to an active AWS account.\nConfigure the AWS CLI by typing aws configure in the command line.\n\nTo list the content of the s3 bucket associated with VOiCES, run\naws s3 ls s3://lab41openaudiocorpus\n\nDownload data using\naws s3 sync &lt;source&gt; &lt;target&gt; [--options]\nor\naws s3 cp &lt;source&gt; &lt;target&gt; [--option]\n\nData Releases\n\n  Fall 2018 : room-1 and room-2, 12 mics, 200 speakers\n  Fall 2019 : room-3 and room-4, 12 mics, 200 speakers\n  Winter 2019: hold-out set\n\n",
    "url": "/downloads/"
  },
  
  {
    "title": "Description",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "\n\nWe are hosting a special session at Interspeech 2019--   \nJOIN THE VOiCES FROM A DISTANCE CHALLENGE HERE!\n \n\n\n\nSRI International and Lab41, In-Q-Tel, are proud to release the Voices Obscured in Complex Environmental Settings (VOICES) corpus, a collaborative effort that brings speech data in acoustically challenging reverberant environments to the researcher. Clean speech was recorded in rooms of different sizes, each having distinct room acoustic profiles, with background noise played concurrently. These recordings provides audio data that better represent real-use scenarios. The intended purpose of this corpus is to promote acoustic research including, but not limited to:\n\n\n  speaker Identification, speech recognition, speaker detection\n  event and background classification, speech/non-speech\n  source separation and localization, noise reduction, general enhancement, acoustic quality metrics\n\n\nThe corpus contains the source audio, the retransmitted audio, orthographic transcriptions, and speaker labels. The ultimate goal of this corpus is to advance acoustic research by providing access to complex acoustic data. The corpus will be released as open source, Creative Commons BY 4.0, free for commercial, academic, and government use.\n\n\n\nDataset Details\n\nThis is one of the largest corpora to date that has transcriptions and simulatenously recorded real-world noise. The details:\n\n\n  Source Material: a total of 15 hours (3,903 audio files)\n  Language audio contains English read speech with male and females\n  Simulated Head Movement the loudspeaker playing the foreground speech was on a motorized rotating platform\n  Distractor Noise a large collection containing television, music, babble noise, and HVAC at various SNR\n  Multiple Rooms large, medium, and small, with various reverberation\n\n\nMore specific details can be seen at in our readme and paper in the reading section\n",
    "url": "/"
  },
  
  {
    "title": "Description",
    "excerpt": "Links and Contacts\n",
    "content": "Utilities\n\n  How to use the VOiCES set? Here’s an example\n  Do you have an issue? Tell us about it here.\n\n\nCompetitions\n\n  UCSD Hackathon Jan 2019\n  Interspeech 2019 Special Session\n\n\nSponsoring Organizations\n\n  \n \n  \n\n\nParticipating Organizations\n\n  MIT Lincoln Laboratory\n  Amazon Web Services\n\n",
    "url": "/links/"
  },
  
  {
    "title": "Organizers",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Zeb Armstrong\n\n\nZeb Armstrong is a Computer Scientist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. His main interest is preparing and transitioning speech technologies to fulfill\nthe needs of government and commercial users through software testing and user interface\ndesign. He uses his experience with operational signals collection and audio processing to\nanticipate clients’ needs and inform design decision.  Armstrong studied software engineering at\nClarkson University before joining the US Army in 2013. While in the Army he worked as an\nArabic linguist and signals intelligence analyst, serving in both tactical and strategic capacities.\n\nMaria Alejandra Barrios\n\n\nDr. Maria Alejandra Barrios is a Senior data scientist at Lab41, In-Q-Tel. Her research\ninterests include deep learning implementations for audio signal processing,\ntime-series analysis, image processing, and computer vision. Dr. Barrios received\nher PhD. in plasma physics from University of Rochester in 2010. Prior to joining\nLab41 Dr. Barrios was a staff scientist at Lawrence Livermore National Laboratory, working\non laser-driven fusion experiments at the National Ignition Facility (NIF).\n\nHoracio Franco\n\n\nHoracio Franco is Chief Scientist in SRI International’s Speech Technology and Research (STAR)\nLaboratory. Since joining SRI in 1990, he has contributed in the areas of acoustic modeling,\nhybrid Hidden Markov Model (HMM)-neural net speech recognition approaches, speech\nrecognizer architectures, speech technology for language learning, noise-robust speech\nrecognition, and speech-to- speech translation systems. He has co-authored more than 70\npapers, as well as several communications and book chapters. He holds several U.S. patents. He\nhas also been active in leading efforts to develop and deploy SRI's speech technology in\ndifferent areas of government and commercial use. His Ph.D. in engineering is from the\nUniversity of Buenos Aires, Argentina.\n\nPaul Gamble\n\n\nDr. Paul Gamble studied Biophysics at the University of Pennsylvania and received an MD from Washington University in St. Louis where he studied nerve-computer interfaces. He joined Lab 41 as a Data Scientist in September 2017 and is helping to develop a pipeline for classifying synthetic DNA sequences.\n\nMartin Graciarena\n\n\nMartin Graciarena is a Technical Manager in SRI International’s Speech Technology and\nResearch (STAR) Laboratory. His research interests include noise robust features; voice activity\ndetection; speaker, language, and speech recognition; non-audible microphone signal\nprocessing; and bird song processing. He has more than 30 publications in peer-reviewed\nconferences and holds two patents. His Ph.D. in noise robust speech processing is from the\nUniversity of Buenos Aires, Argentina.\n\nJeff Hetherly\n\n\nDr. Jeff Hetherly Dr. Jeff Hetherly is a data scientist at NVIDIA. His background is in scientific statistical analysis, methods for dimensionality reduction, and machine learning. Dr. Hetherly received his Ph.D. in experimental particle physics from Southern Methodist University where he spent two years located at CERN analyzing data for the ATLAS experiment.\n\nAaron Lawson\n\n\nAaron Lawson is Assistant Director of SRI International’s Speech Technology and Research\n(STAR) Laboratory. His research interests include voice forensics and biometrics, language and speaker identification from speech, social media information extraction, noise robustness, and fielding systems. He has published more than 30 papers covering speech, natural language\nand linguistics. His Ph.D. in applied linguistics is from Cornell University.\n\nKarl Ni\n\n\nDr. Karl Ni is a Senior Software Engineer at Google. His interests lie in computer vision, audio signal processing, speech recognition, and natural language processing. Prior Dr. Ni served as principle investigator and program manager on laboratory directed projects at federally funded research and development centers, both MIT Lincoln Laboratory and Lawrence Livermore National Laboratory. Projects started and led at these organizations resided under the President’s BRAIN initiative, the Departments of Defense and Homeland Security, and internally funded research. Dr. Ni received his doctoral degree from the University of California, San Diego in Pattern Recognition Techniques for Image Processing, and he received his bachelors degree from the University of California at Berkeley.\n\nColleen Richey\n\n\nColleen Richey is an Advanced Linguist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. Her research interests include automatic recognition of conversational\nspeech, machine translation, language education, and speech and language technology for low-\nresource languages. She has published more than 20 papers in speech technology and machine\ntranslation. Her M.A. in linguistics is from Stanford University.\n\nAllen Stauffer\n\n\nAllen Stauffer is a Computer Scientist in SRI International’s Speech Technology and Research\n(STAR) Laboratory. His main focus is preparing speech processing technologies for commercial\nand government transition, including software and hardware testing and validation, system\nevaluation and preparation of relevant data to meet client needs. A second facet of his work is\nin developing speech processing system requirements based on his understanding of client\nneeds and interactions with the operational community. His M.A. in computer science is from\nthe University of Texas at Dallas.\n\nCory Stephenson\n\n\nDr. Cory Stephenson is a data scientist at Nervana, InTel.  His background is in complex systems and non-equilibrium thermodynamics.  His most recent work is in the application of machine learning and deep learning methods to audio signal processing. Dr. Stephenson received his Ph.D. from the University of Illinois at Urbana-Champaign.\n",
    "url": "/organizers/"
  },
  
  {
    "title": "Reading",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Dataset Description\n\nThe Voices Obscured in Complex Environmental settings (VOiCES) corpus presents audio\nrecorded in acoustically challenging conditions. Recordings took place in real rooms of\nvarious sizes, capturing different background and reverberation profiles for each\nroom. Various types of distractor noise (TV, music, or babble) were simultaneously\nplayed with clean speech. Audio was recorded at a distance using twelve microphones\nstrategically placed throughout the room. To imitate human behavior during conversation,\nthe foreground speaker used a motorized platform, rotating over a range of angles during recordings.\n\nThree hundred distinct speakers from LibriSpeech’s “clean” data subset were selected as the source audio, ensuring a 50-50 female-male split. In preparation for\nupcoming data challenges, the first release of the VOiCES corpus will include 200 speakers only. The remaining 100 speakers will be reserved for model validation; the full corpus\n(300 speakers) will be released once the data challenge is closed.\n\nIn addition to the full dataset, we also provide a dev set and a mini-dev set. Both maintain the data structure of the VOiCES corpus, but include a small subset of data. The dev set includes audio files for four randomly selected speakers (50-50 female-male split) for data recorded in Room-1. This includes data from all twelve microphones. The mini-dev set includes one speaker, one room (Room-1), and studio microphones only.\n\nReadme\n\nFor more details about how to use the dataset, please see our README.\n\nBlog Posts\n\n\n  Introducing the Voices Obscured in Complex Environmental Settings (VOiCES) corpus\n\n\nPublications\n\n\n  Robust Speaker Recognition from Distant Speech Under Real Reverberant Environments\n  Corpus Description and Collection Methodology\n  175th Meeting of the Acoustical Society of America\n\n",
    "url": "/reading/"
  },
  
  {
    "title": "Reading",
    "excerpt": "A short description of the VOiCES corpus\n",
    "content": "Recording Room Configurations\nData for VOiCES was recorded in real rooms. The full corpus includes various rooms, each with unique acoustic profiles. Below are diagrams of room configurations, dimensions, and microphone placement for released data.\n\nFor all diagrams, the main loudspeaker (foreground speech) is shown at its 90 deg position, as the orange rectangle. Distractor speakers (replaying TV, music, or babble noise) are shown as the blue squares. Circles depict microphones: small light green circles are lapel microphones, large dark circles are studio microphones. Details on microphone ID numbers can be found in the README.\n\nRoom-1\n\nRoom dimensions : 146’’ x 107’’\n\nRoom-2\n\nRoom dimensions: 225’’ x 158 ‘’\n",
    "url": "/rooms/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  },
  
  {
    "title": "Dataset Description",
    "excerpt": "\n",
    "content": "Dataset Description\n\nThe VOiCES corpus is a collaboration between\nSRI International and Lab41, In-Q-Tel, presenting audio\nrecorded in acoustically challenging conditions. Recordings took place in real rooms of\nvarious sizes, capturing different background and reverberation profiles for each\nroom. Various types of distractor noise were simultaneously\nplayed with clean speech. Audio was recorded at a distance using various microphones\nplaced throughout the room. To imitate human behavior during conversation,\nthe foreground loudspeaker was placed on a motorized platform that rotated over a range of angles during recordings.\n\nThree hundred distinct speakers from LibriSpeech’s “clean” data subset were selected as the source audio, ensuring a 50-50 female-male split. In preparation for\nupcoming data challenges, the first release of the VOiCES corpus will include 200 speakers only. The remaining 100 speakers will be reserved for model validation; the full corpus\n(300 speakers) will be released once the data challenge is closed.\n\nSource audio references\n\nSource audio references, per LibriSpeech, are provided in three different tables as follows:\n\nInformation on the speaker ID, book ID, and chapter ID\n\n  Lab41-SRI-VOiCES-speaker-book-chapter.tbl\n\n\nSpeaker ID, gender, and LibriSpeech data subset\n\n  Lab41-SRI-VOiCES-speaker-gender-dataset.tbl\n\n\nOrthographic transcription of all audio files\n\n  Lab41-SRI-VOiCES.refs\n\n\nData format\n\nAudio files are available in WAV format with 16 kHz sample rate with 16-bit precision. All files begin with the corpus name Lab41-SRI-VOiCES. Source audio files specify speaker, chapter, and chapter segment identification number. The file naming format sample is shown below:\n\n  Lab41-SRI-VOiCES-src-sp&lt; speaker_ID &gt;-ch&lt; chapter_ID &gt;-sg&lt; segment_ID &gt;.wav\n\n\nThe naming convention for audio recorded at a distance includes all the above information, with additional descriptors for room, distractor noise, microphone type, microphone location, and position of foreground loudspeaker in degrees. The file naming format is shown below:\n\n  Lab41-SRI-VOiCES-&lt; room &gt;-&lt; distractor_noise &gt;-sp&lt; speaker_ID &gt;-ch&lt; chapter_ID &gt;-seg&lt; segment_ID &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;-dg&lt; degree &gt;.wav\n\n\nAudio files to characterize the room response are also available:\n\n  Lab41-SRI-VOiCES-&lt; room &gt;-&lt; signal &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;.wav\n\n\nAs are recordings of distractor noise only or ambient room background only:\n\n  Lab41-SRI-VOiCES-&lt; distractor_noise &gt;-mc&lt; mic_ID &gt;-&lt; mic_type &gt;-&lt; mic_location &gt;.wav\n\n\n\nPossible descriptors for room, distractor noise, microphone type, and microphone location, are show in the table below.\n\n\n\n\n  \n    \n      File Code\n      Type\n      Definition\n    \n  \n  \n    \n      rm1\n      Room\n      Room-1: dimensions 146” x 107” (x 107” height)\n    \n    \n      rm2\n      Room\n      Room-2: dimensions 225” x 158” (x 109” height)\n    \n    \n      scr\n      Source audio\n      Source audio for foreground speaker\n    \n    \n      none\n      Distractor noise\n      No distractor noise played\n    \n    \n      musi\n      Distractor noise\n      Music distractor noise played\n    \n    \n      tele\n      Distractor noise\n      Television distractor noise played\n    \n    \n      babb\n      Distractor noise\n      Babble distractor noise played\n    \n    \n      stu\n      Mic type\n      Cardioid dynamic studio microphone\n    \n    \n      lav\n      Mic type\n      Omnidirectional condenser lavalier microphone\n    \n    \n      clo\n      Mic location\n      Closest to foreground speaker- on table\n    \n    \n      mid\n      Mic location\n      Mid-distance to foreground speaker- on table\n    \n    \n      far\n      Mic location\n      Farthest to foreground speaker- on stand\n    \n    \n      beh\n      Mic location\n      Behind foreground speaker- on stand\n    \n    \n      cec\n      Mic location\n      Overhead on ceiling, clear\n    \n    \n      ceo\n      Mic location\n      Overhead on ceiling, fully obstructed\n    \n    \n      tbo\n      Mic location\n      Partially obstructed - table\n    \n    \n      wal\n      Mic location\n      Fully obstructed - wall\n    \n    \n      impulse\n      Signal\n      Two seconds with transient sound in middle, for room response\n    \n    \n      swoop\n      Signal\n      Rising tone for 20 seconds, for room response\n    \n    \n      tone\n      signal\n      Steady tone for 15 seconds, for room response\n    \n  \n\n\n\n\nAll the data is contained in two main folders: distant-16k, containing all the audio recordings, and source-16k, containing\nthe audio files used from LibriSpeech, corrected for DC offset and normalized to each file’s peak amplitude. The WAV files for the source audio are organized in subdirectories by speaker ID. The distant-16k has three main subdirectories:\n\n  distractors : distractor noise recordings with no foreground audio for all rooms\n  room-response : recorded sound to determine room-response for all rooms\n  speech : for each room, recordings of foreground audio with babble, music, television or no distractor noise, arranged by\nspeaker ID in each subfolder.\n\n\nThe directory hierarchy is shown below :\n\n\n  \n\n\n\nMicrophone Details\n\nMicrophone identification numbers are unique to a specific microphone location and type, defined below.\n\n\n\n\n  \n    \n      Mic_ID\n      Location\n      Model\n      Type\n    \n  \n  \n    \n      01\n      clo\n      SHURE SM58\n      stu\n    \n    \n      02\n      clo\n      AKG 417L\n      lav\n    \n    \n      03\n      mid\n      SHURE SM58\n      stu\n    \n    \n      04\n      mid\n      AKG 417L\n      lav\n    \n    \n      05\n      far\n      SHURE SM58\n      stu\n    \n    \n      06\n      far\n      AKG 417L\n      lav\n    \n    \n      07\n      beh\n      SHURE SM58\n      stu\n    \n    \n      08\n      beh\n      AKG 417L\n      lav\n    \n    \n      09\n      tbo\n      AKG 417L\n      lav\n    \n    \n      10\n      cec\n      AKG 417L\n      lav\n    \n    \n      11\n      ceo\n      AKG 417L\n      lav\n    \n    \n      12\n      wal\n      SHURE SM11\n      lav\n    \n  \n\n\n\nDistance (inches) between microphones and loudspeakers or floor, for Room-1 and Room-2 recordings.\n\n\n\n\n\n\n\n\n  \n     \n     Foreground \n     Distractor 1 \n     Distractor 2 \n     Distractor 3\n     Floor     \n  \n  \n    Mic_ID\n    rm-1\n    rm-2\n    rm-1\n    rm-2\n    rm-1\n    rm-2\n    rm-1\n    rm-2\n    rm-1\n    rm-2\n  \n  \n     01 \n     38 \n     80 \n     71 \n     112 \n     71 \n     84 \n     53 \n     64 \n     42 \n     39   \n  \n  \n     02  \n     38 \n     80  \n     71 \n     112 \n     71 \n     84 \n     53  \n     64  \n     42 \n     39   \n  \n  \n     03 \n     72 \n     131 \n     35 \n     81  \n     56 \n     58  \n     52 \n     95  \n     42  \n     39    \n  \n  \n     04 \n     72  \n     131 \n     35 \n     81  \n     56 \n     58  \n     52 \n     95  \n     42  \n     39    \n  \n  \n     05  \n     119 \n     228 \n     72 \n     101 \n     33 \n     104 \n     83 \n     186 \n     70 \n     70   \n  \n  \n     06 \n     119 \n     228 \n     72 \n     101 \n     33 \n     104 \n     83 \n     186 \n     70 \n     70   \n  \n  \n     07 \n     29 \n     29 \n     115 \n     193 \n     133 \n     170 \n     94 \n     94  \n     70  \n     70   \n  \n  \n     08 \n     29 \n     29 \n     115 \n     193 \n     133 \n     170 \n     94 \n     94 \n     70 \n     70   \n  \n  \n     09 \n     58 \n     109 \n     64 \n     98 \n     60 \n     65 \n     49 \n     82 \n     28 \n     25   \n  \n  \n     10 \n     75 \n     128 \n     90 \n     107 \n     108 \n     103 \n     106 \n     104 \n     105 \n     105   \n  \n  \n     11 \n     75 \n     128 \n     90 \n     107 \n     108 \n     103 \n     106 \n     104 \n     106 \n     106   \n  \n  \n     12 \n     130 \n     116 \n     861 \n     116 \n     40 \n     115 \n     81 \n     164 \n     12 \n     10   \n  \n\n\n\n\n\nLicensing\n\nVOiCES is publicly available released under Creative Commos BY 4.0, free for commercial, academic, and\ngovernment use. Please do reference VOiCES if using the data in\npublications.\n",
    "url": "/Lab41-SRI-VOiCES_README/"
  },
  
  {
    "title": "VOiCES Challenge 2019 FAQ’s",
    "excerpt": "\n",
    "content": "VOiCES Challenge 2019 FAQ’s\n\nQ1. Am I allowed to use additional “non-speech” data for data augmentation by mixing them with clean training\ndata you provided for the fixed condition?\nAs specified in the evaluation plan, you are allowed to use any “non-speech” data for data augmentation\nin the fixed condition for ASR and speaker recognition.\n\nQ2. I cannot download the data, the website is blocked by my institution or country. How can I get the\ndata?\nPlease send an email to voices_poc@sri.com\nand we will create an ftp link for you to download the data.\n\nQ3. Is there a registration deadline?\nParticipants may register at any time before March 4th.\n\nQ4. Will the accepted paper submitted for the challenge be published in the conference proceedings?\nYes, the submitted paper will go through a peer review process and will be part of the official Interspeech\nconference proceedings.\n\nQ5. Will there be metadata available in the evaluation set (speaker, room, microphone, etc) for each utterance (as in the dev set)?\nWe only provide metadata in the dev set as a way for the participants to look at the performance of\ntheir system under different microphones, rooms or distractors. The metadata will not be provided for the evaluation set, in fact, we anticipate the file names will be fully anonymized.\n\nQ6. We are allowed to use VoxCeleb1, VoxCeleb2 and SITW data for training. Are we limited to using the official annotations only? For example, the VoxCeleb corpora contain segments from YouTube videos. Are we limited to using only the annotated segments or may we use other parts of the videos also, if we find it useful?\nThe audio data from the Voxceleb1 and Voxceleb2 is restricted to the official annotations for the fixed condition submissions. In this way, the fixed condition can serve its purpose of measuring the performance of different systems trained with the same data (or a subset thereof). However, you can certainly use all the audio from the Voxceleb-provided URLs for\nthe open condition system development.\n\nQ7. How are we supposed to submit our system description and score files?\nWe will provide you instructions along with the release of the evaluation data on how to upload the\nsystem description and score files.\n\nQ8. For the ASR task, may we use an external pronunciation lexicon (e.g., CMUdict) for deriving the pronunciation of words in the training data?\nYes, you may use any dictionary you have for the purpose of this evaluation, in either fixed or open\ncondition.\n\nQ9. How will the team submission be ranked?\nThe official score for a team will be selected as the best primary metric from systems submitted by\nthe team for that condition (up to 3 systems can be submitted per condition per task per team). These official scores will be used for ranking teams.\n",
    "url": "/VOiCES_InterspeechChallenge2019_FAQ/"
  }
  
]

